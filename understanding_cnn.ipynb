{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Convolutional Neural Networks (CNN)__ are locally connected, each node is connected to amall subset of the previous units. And it can learn the object features such as a car wheel or a cat nose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRwed5zvnSDt0zrFd_gf-kUIMoF7Nm6FXIwDw&usqp=CAU\" width=\"1000\" height=\"341\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url='https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRwed5zvnSDt0zrFd_gf-kUIMoF7Nm6FXIwDw&usqp=CAU',\n",
    "      width=1000,\n",
    "      height=341)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Convolutional Neural Networks (CNN) has an input layer and a stack of hidden layers then the output layer just like the Fully Connected Network, the weights are randomly inialized, activation function is applied, we use back propagation and optimization algorithms to update the weights. The CNN does not need FLatten layer, it takes the whole image matrix as an input.\n",
    "\n",
    "The network consist of 2 main parts:\n",
    "\n",
    "1- Feature extraction part\n",
    "\n",
    "2- classification part\n",
    "\n",
    "* The image is passed to the CNN layers to extract features that called feature map, through this process the image dimension will shrinks and the number of feature maps will increase.\n",
    "\n",
    "* The output of the final layer in the CNN is the feature that was extracted form the image and it represent the image and also this is what we are going to classify. Flatten layer used to flatten the feature map and then pass it to a fully connected layer for classification.\n",
    "\n",
    "* The earlier layers in the CNN will learn basic features such as edges and the later layers will learn more complex features such as a wheel of a car or the nose of a cat.\n",
    "\n",
    "So to summarize what we learn until now, we have an input image that passed directly to the network without flattening it, the the first part (the feature extractor) extract features that represent the input image and give us what we call it a feature map , and finally this feature map to the classifier that consists of Fully connected layers that perform the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://www.researchgate.net/profile/Lavender-Jiang-2/publication/343441194/figure/fig2/AS:921001202311168@1596595206463/Basic-CNN-architecture-and-kernel-A-typical-CNN-consists-of-several-component-types.ppm\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://www.researchgate.net/profile/Lavender-Jiang-2/publication/343441194/figure/fig2/AS:921001202311168@1596595206463/Basic-CNN-architecture-and-kernel-A-typical-CNN-consists-of-several-component-types.ppm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network consists of 2 main parts: the CNN and the FCN\n",
    "\n",
    "The CNN consist of two main components:\n",
    "\n",
    "    1- Convolutional Layer (CONV)\n",
    "    2- Pooling Layer  (POOL)\n",
    "    \n",
    "__INPUT > CONV > POOL > CONV > POOL > CONV > POOL > FC > FC (softmax)__\n",
    "\n",
    "So the full architecture consists of Convolutional Layers , Pooling Layers and Fully Connected Layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Convolutional Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional Layer is the basis of the Convolutional Networks, the units here is the filters and also called kernels and they works by sliding the filter over the input image, apply some processing at each location and store the result in the new processed image as we see in the figure below.\n",
    "\n",
    "In CNN these filters are the weights, their values are intialized randomly and learned during the training process. The area of the image that the filter process on is called receptive field.\n",
    "\n",
    "The math here is very simple, at each location the filter stop we multiply each pixel value in the receptive field by the corresponding pixel in the filter and then sum them to get the value of the center pixel in the new image as we see in the figure below.\n",
    "\n",
    "If the images are colored lets say with size of (520, 520, 3) where 3 is the number of channels (Red, Green and Blue (RGB)) the filter also will be (3, 3, 3) where the last 3 is number of channels in the input image. We have filter of size (3,3) for each channel in the input image and for each channel we do the same calculation then add the results of the filter processing on the 3 channels.\n",
    "\n",
    "In sime application colors are very important to identify objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://i.stack.imgur.com/CQtHP.gif\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://i.stack.imgur.com/CQtHP.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filters are parameters that learned during the training process. some filters can detect vertical edges while other might detect horizontal edges and other detect different things.\n",
    "\n",
    "so we can see that each filter produce its own feature map, the conv layer has many of those filters so the number of filters in the conv layer determine the number of feature maps that produced after applying this layer on the input. And the number of filters represent the depth of the output of the layer. As we increase the number of filters, the complexity of the network increase and enable use to detect more complex features.\n",
    "\n",
    "This kernel is just a matrix of weights that are learned during the training process this filter is works by sliding over the image to extract features. kernels are almost always squares and can have different sizes (3x3), (5x5), (7x7) it is an hyperparameter that you can tune and the performance will be different based on the problem that you are solving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Strides and Padding__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://miro.medium.com/max/658/0*jLoqqFsO-52KHTn9.gif\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://miro.medium.com/max/658/0*jLoqqFsO-52KHTn9.gif')\n",
    "\n",
    "#Source: https://towardsdatascience.com/cnn-part-i-9ec412a14cb1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the figure above the conv operation output is smaller that the input, we can control the shape of the output of this operation by the __stride and padding__\n",
    "\n",
    "Stride is the amount by which the filter slides over the image, for example if stride = 1 then the filter will slide one pixel at a time, and if stride = 2 it will slide 2 pixel at a time.\n",
    "\n",
    "padding: We add zeros around the border of the image to preserve the spatial size of the image so we can train a deeper network and prevent losing information from the edges of the image.\n",
    "\n",
    "the values for padding is either 2: 'same' where we add zeros around the image border in which the size of the output image is the same as the input image. 2: 'valid' which means without padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://res.cloudinary.com/practicaldev/image/fetch/s--nUoflRuG--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://i.ibb.co/kG5vPdn/final-cnn.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://res.cloudinary.com/practicaldev/image/fetch/s--nUoflRuG--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://i.ibb.co/kG5vPdn/final-cnn.png')\n",
    "\n",
    "#Source: https://towardsdatascience.com/cnn-part-i-9ec412a14cb1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So to __summarize__ what we learn until now, the first component of the CNN is the Conv layer, in this layer we have a lot of hyperparameters that we can tune to control the output of it. The Conv layers contain filters (kernels) that contain the weights that learned during training, these filters works by sliding them over the image and each filter detect a certain feature, where the filters in the first layers detect simple things like edges and when we go deeper in the network, the filters will detect more complex features. Each filter in the layer produce a feature map and the feature maps of all filters in the layer are concatenated and this is the output of the layer. to control the shape of th output of the layer we have two additional hyperparameters, the stride which control the amount by which the filter slides over the image and the padding where we add zeros around the border of the image.\n",
    "\n",
    "Example: if the input image is 224,224,3 to a Conv layer with 64 filters of size (3,3), stride of 1, and no padding ('valid'). The output will be of size {(224 + 0 -3 /1) +1 = 222} (222, 222, 64) where the 3 in the input image size is the RGB channels and the 64 in the output means 64 feature map produced by the 64 filters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Pooling Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we go deeper in the ConvNet the number of the feature maps (depth) is increase which lead to the increase to the number of parameters that will increase the computational power and memory needed.\n",
    "\n",
    "Pooling layer reduce the size of the feature map (not the depth it reduce the height and the width) with two types of pooling max pooling and Average pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://nico-curti.github.io/NumPyNet/NumPyNet/images/maxpool.gif\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://nico-curti.github.io/NumPyNet/NumPyNet/images/maxpool.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pooling we also have a window with specified size that sliding on the feature map but without weights, the window take the max pixel value and ignore the rest values in max pooling, or take the average of the values in average pooling, so the pooling layer help us to keep the important information and pass them to the next layer.\n",
    "\n",
    "__Where we put the pooling layers in the CNN?__\n",
    "\n",
    "usually pooling layer are placed after one or two convolutional layers.\n",
    "\n",
    "the output of the final pooling layer after a sequence of conv and pool layers will be for example (7, 7, 120) where the 120 id the depth (number of feature maps) now this output is ready for classification but first we need to flatten it then feeding it to the fully connected layers.\n",
    "\n",
    "Pooling layers don't have any parameters to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Fully Conected Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pass the image through our feature extractor that composed of convolutional and pooling layer and give us the feature that we will pass through the FCL for classification.\n",
    "\n",
    "in This stage, we flatten the output of the feature extractor lets say for example its (7, 7, 40) wen we flattening it we get a vector of size 7740 = 1960, then we feed this vector to a fully connected layers (we can use for example one layer with 256 units with relu activation function and other layer with 9 units with softmax activation function for classification)\n",
    "\n",
    "the number of units in the last layer will equal to the number of classes where each node represents the probability of each class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
